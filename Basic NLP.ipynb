{
 "metadata": {
  "name": "",
  "signature": "sha256:9c237ef39fb928342a76f5b2c76e5a1e257ea13fbca78a8f572a1eadb97bd90f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# NLP IN PYTHON\n",
      "***\n",
      "\n",
      "## Introduction\n",
      "Open source software libraries like the [Natural Language Toolkit for Python (NLTK)](http://www.nltk.org/) is mainly used for NLP.\n",
      "\n",
      "### Requirements\n",
      "\n",
      "We'll make use of the following Python packages in the example code:\n",
      "\n",
      "- [nltk](http://www.nltk.org/install.html)\n",
      "- [BeautifulSoup4](http://www.crummy.com/software/BeautifulSoup/) \n",
      "- [scikit-learn](http://scikit-learn.org/stable/install.html)\n",
      "\n",
      "\n",
      "#### Java libraries \n",
      "One of the examples will use NLTK's interface to the [Stanford Named Entity Recognizer](http://www-nlp.stanford.edu/software/CRF-NER.shtml#Download), which is distributed as a Java library. In particular, you'll want the following files handy in order to run this particular example:\n",
      "\n",
      "- stanford-ner.jar\n",
      "- english.all.3class.distsim.crf.ser.gz\n",
      "\n",
      "***\n",
      "\n",
      "## Getting Started\n",
      "The first thing we'll need to do is <code>import nltk</code>:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Downloading NLTK resources\n",
      "The first time you run anything using NLTK, you'll want to go ahead and download the additional resources that aren't distributed directly with the NLTK package. Upon running the <code>nltk.download()</code> command below, the the NLTK Downloader window will pop-up. In the Collections tab, select \"all\" and click on Download. As mentioned earlier, this may take several minutes depending on your network connection speed, but you'll only ever need to run it a single time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text='In the middle of a raging political row over the release of Kashmiri separatist Masarat Alam from jail, the ruling BJP has reportedly warned its Jammu and Kashmir ally Mufti Mohammad Sayeed: \"Mend your ways or we will end the alliance.\" BJP president Amit Shah summoned party leader Nirmal Singh, who is the state\\'s Deputy Chief Minister, and asked him to deliver a stern message to Mr Sayeed and his PDP, say sources. The BJP reportedly said that it would not sacrifice its ideology for the sake of power. \\\"Next time, there will be consequences,\\\" Nirmal Singh was reportedly asked to tell the PDP. The days-old alliance between ideological opposites PDP and the BJP has been pushed to the edge by a series of controversies since the coalition government came to power on March 1. The row over separatist Masarat Alam\\'s release from jail on Saturday has left the Centre facing opposition attacks for the third time in two weeks. On Monday, Prime Minister Narendra Modi said that his Jammu and Kashmir ally had freed the hardliner without consulting the Centre. \"I share the opposition\\'s aakrosh (anger),\" he had said in the Lok Sabha. But letters exclusively accessed by NDTV have revealed that the decision to free Alam was taken when Jammu and Kashmir was under central rule. In one letter, written on February 4, the state home department had flagged that a detention order against Masarat Alam had lapsed and he could not be kept in jail without any charges. No fresh charges were brought and a letter on March 4 said that Alam\\'s preventive detention had \"not been approved by the government\" and he should be released. The PDP said Union Home Minister Rajnath Singh should have known about the decision to release Alam. \\\"There is in the public domain a letter that the release was ordered on February 2 when there was governor\\'s rule here, so the union home minister should know about it,\\\" said PDP s Haseeb Drabu, the Jammu and Kashmir Finance Minister.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Frequency Analysis\n",
      "\n",
      "This is a little snippent to preprocess and find the frequency of individual terms. It is necessary to split documents into sentences and then split sentences to tokens.\n",
      "The below code does that.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
      "\n",
      "for token in sorted(set(tokens))[:30]:\n",
      "    print token + ' [' + str(tokens.count(token)) + ']'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "'' [5]\n",
        "'s [5]\n",
        "( [1]\n",
        ") [1]\n",
        ", [13]\n",
        ". [13]\n",
        "1 [1]\n",
        "2 [1]\n",
        "4 [2]\n",
        ": [1]\n",
        "Alam [6]\n",
        "Amit [1]\n",
        "BJP [4]\n",
        "But [1]\n",
        "Centre [2]\n",
        "Chief [1]\n",
        "Deputy [1]\n",
        "Drabu [1]\n",
        "February [2]\n",
        "Finance [1]\n",
        "Haseeb [1]\n",
        "Home [1]\n",
        "I [1]\n",
        "In [2]\n",
        "Jammu [4]\n",
        "Kashmir [4]\n",
        "Kashmiri [1]\n",
        "Lok [1]\n",
        "March [2]\n",
        "Masarat [3]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Word Stemming\n",
      "[Stemming](http://en.wikipedia.org/wiki/Stemming) is the process of reducing a word to its base/stem/root form. Most stemmers are pretty basic and just chop off standard affixes indicating things like tense (e.g., \"-ed\") and possessive forms (e.g., \"-'s\"). Here, we'll use the Snowball stemmer for English, which comes with NLTK.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.snowball import SnowballStemmer\n",
      "\n",
      "stemmer = SnowballStemmer(\"english\")\n",
      "stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
      "\n",
      "for token in sorted(set(stemmed_tokens))[50:75]:\n",
      "    print token + ' [' + str(stemmed_tokens.count(token)) + ']'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "drabu [1]\n",
        "edg [1]\n",
        "end [1]\n",
        "exclus [1]\n",
        "face [1]\n",
        "februari [2]\n",
        "financ [1]\n",
        "flag [1]\n",
        "for [2]\n",
        "free [1]\n",
        "freed [1]\n",
        "fresh [1]\n",
        "from [2]\n",
        "govern [2]\n",
        "governor [1]\n",
        "had [5]\n",
        "hardlin [1]\n",
        "has [3]\n",
        "haseeb [1]\n",
        "have [2]\n",
        "he [3]\n",
        "here [1]\n",
        "him [1]\n",
        "his [2]\n",
        "home [3]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Lemmatization\n",
      "\n",
      "There might be some irregular words that might not have worked with stemmers. Therefore, we consider a dictionary based methods to derive canonical forms of the words called 'lemmas'.  For example, *run*, *runs*, *ran*, and *running* all correspond to the lemma *run*.\n",
      "\n",
      "The example below compares the output of the Snowball stemmer with the WordNet lemmatizer (also distributed with NLTK). Notice that the lemmatizer correctly converts *women* into *woman*, while the stemmer turns *lying* into *lie*. Additionally, both replace *eyes* with *eye*, but neither of them properly transforms *told* into *tell*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmatizer = nltk.WordNetLemmatizer()\n",
      "temp_sent = \"Several women told me I have lying eyes.\"\n",
      "\n",
      "print [stemmer.stem(t) for t in nltk.word_tokenize(temp_sent)]\n",
      "print [lemmatizer.lemmatize(t) for t in nltk.word_tokenize(temp_sent)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'sever', u'women', u'told', 'me', 'i', u'have', u'lie', u'eye', '.']\n",
        "['Several', u'woman', 'told', 'me', 'I', 'have', 'lying', u'eye', '.']\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### NLTK Frequency Distributions\n",
      "Code below displays the top 25 tokens appearing most frequently in the text of our article."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist = nltk.FreqDist(stemmed_tokens)\n",
      "\n",
      "for item in fdist.items()[:25]:\n",
      "    print item"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'edg', 1)\n",
        "(u'ndtv', 1)\n",
        "(u'opposit', 3)\n",
        "(u'over', 2)\n",
        "(u'approv', 1)\n",
        "(u'report', 3)\n",
        "(u'row', 2)\n",
        "(u'middl', 1)\n",
        "('(', 1)\n",
        "(u'had', 5)\n",
        "(',', 13)\n",
        "(u'alli', 2)\n",
        "(u'should', 3)\n",
        "(u'here', 1)\n",
        "('to', 7)\n",
        "(u'decis', 2)\n",
        "(u'under', 1)\n",
        "(u'mend', 1)\n",
        "(u'has', 3)\n",
        "(u'mufti', 1)\n",
        "(u'sourc', 1)\n",
        "(u'march', 2)\n",
        "(u'minist', 5)\n",
        "(u'jammu', 4)\n",
        "(u'drabu', 1)\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Filtering out Stop Words\n",
      "Notice in the output above that most of the top 25 tokens are worthless. With the exception of things like *facebook*, *content*, *user*, and perhaps *emot* (emotion?), the rest are basically devoid of meaningful information. They don't really tells us anything about the article since these tokens will appear is just about any English document. What we need to do is filter out these [*stop words*](http://en.wikipedia.org/wiki/Stop_words) in order to focus on just the important material.\n",
      "\n",
      "While there is no single, definitive list of stop words, NLTK provides a decent start. Let's load it up and take a look at what we get:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(nltk.corpus.stopwords.words('english'))[:25]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "[u'a',\n",
        " u'about',\n",
        " u'above',\n",
        " u'after',\n",
        " u'again',\n",
        " u'against',\n",
        " u'all',\n",
        " u'am',\n",
        " u'an',\n",
        " u'and',\n",
        " u'any',\n",
        " u'are',\n",
        " u'as',\n",
        " u'at',\n",
        " u'be',\n",
        " u'because',\n",
        " u'been',\n",
        " u'before',\n",
        " u'being',\n",
        " u'below',\n",
        " u'between',\n",
        " u'both',\n",
        " u'but',\n",
        " u'by',\n",
        " u'can']"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can use this list to filter-out stop words from our list of stemmed tokens before we create the frequency distribution. You'll notice in the output below that we still have some things like punctuation that we'd probably like to remove, but we're much closer to having a list of the most \"important\" words in our article."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmed_tokens_no_stop = [stemmer.stem(t) for t in stemmed_tokens if t not in nltk.corpus.stopwords.words('english')]\n",
      "\n",
      "fdist2 = nltk.FreqDist(stemmed_tokens_no_stop)\n",
      "\n",
      "for item in fdist2.items()[:25]:\n",
      "    print item"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'edg', 1)\n",
        "(u'ndtv', 1)\n",
        "(u'opposit', 3)\n",
        "(u'approv', 1)\n",
        "(u'row', 2)\n",
        "(u'middl', 1)\n",
        "('(', 1)\n",
        "(',', 13)\n",
        "(u'alli', 2)\n",
        "('4', 2)\n",
        "(u'mend', 1)\n",
        "(u'mufti', 1)\n",
        "(u'sourc', 1)\n",
        "(u'march', 2)\n",
        "(u'minist', 5)\n",
        "(u'jammu', 4)\n",
        "(u'drabu', 1)\n",
        "(u'relea', 5)\n",
        "(u'financ', 1)\n",
        "(u'know', 1)\n",
        "(u'govern', 2)\n",
        "(u'report', 3)\n",
        "(u'singh', 3)\n",
        "(u'prime', 1)\n",
        "(u'reveal', 1)\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Named Entity Recognition\n",
      "In a text document, one of the important tasks is named entity recogniton (NER) - extracting the names of persons, places, organizations, and potentially other entity types out of unstructured text. Building an NER classifier requires lots of annotated training data and some [machine learning algorithms](http://en.wikipedia.org/wiki/Conditional_random_field), but fortunately, NLTK comes with a pre-built/pre-trained NER classifier ready to extract entities right out of the box. This classifier has been trained to recognize PERSON, ORGANIZATION, and LOCATION entity types.\n",
      "\n",
      "We're defining a method to perform the following steps:\n",
      "\n",
      "- take a string as input\n",
      "- tokenize it into sentences\n",
      "- tokenize the sentences into words\n",
      "- add part-of-speech tags to the words using <code>nltk.pos_tag()</code>\n",
      "- run this through the NLTK-provided NER classifier using <code>nltk.ne_chunk()</code>\n",
      "- parse these intermediate results and return any extracted entities\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_entities(text):\n",
      "    entities = []\n",
      "    for sentence in nltk.sent_tokenize(text):\n",
      "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
      "        print chunks\n",
      "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'node')])\n",
      "    return entities\n",
      "\n",
      "extract_entities('My name is Charlie and I work for Google in New York. ')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  My/PRP$\n",
        "  name/NN\n",
        "  is/VBZ\n",
        "  (PERSON Charlie/NNP)\n",
        "  and/CC\n",
        "  I/PRP\n",
        "  work/VBP\n",
        "  for/IN\n",
        "  (GPE Google/NNP)\n",
        "  in/IN\n",
        "  (GPE New/NNP York/NNP)\n",
        "  ./.)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag.stanford import NERTagger\n",
      "\n",
      "# change the paths below to point to wherever you unzipped the Stanford NER download file\n",
      "st = NERTagger('/Users/pralav/Documents/Projects/Owned/DeepWiki/stanford-ner/classifiers/english.conll.4class.distsim.crf.ser.gz',\n",
      "               '/Users/pralav/Documents/Projects/Owned/DeepWiki/stanford-ner/stanford-ner.jar', 'utf-8')\n",
      "\n",
      "for i in st.tag('Lucy works at Google in Washington.'.split()):\n",
      "    print '[' + i[1] + '] ' + i[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[PERSON] Lucy\n",
        "[O] works\n",
        "[O] at\n",
        "[ORGANIZATION] Google\n",
        "[O] in\n",
        "[LOCATION] Washington\n",
        "[O] .\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}