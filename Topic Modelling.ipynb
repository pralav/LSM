{
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "name": "",
  "signature": "sha256:e3ce56479d3c0d285e66248a2be1672ec07b3aa84383f12e524ddba6dd1b77ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Topic Modelling Example\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Requirements: \n",
      "    - Scipy \n",
      "    - Numpy\n",
      "    - gensim\n",
      "###Installation Steps:\n",
      "    - easy_install numpy\n",
      "    - easy_install scipy\n",
      "    - easy_install gensim"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataset used are the titles and topic codes from the NYTimes dataset that comes with the RTextTools library in R. It consists of titles from NYTimes front page news and associated codes according to Amber Boydstun's classification scheme.\n",
      "\n",
      "Download data from:\n",
      "https://github.com/timjurka/RTextTools/blob/master/RTextTools/data/NYTimes.csv.gz"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gensim can be applied to large datasets as it incorporates both the online version of LDA and distributed computing capability.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Import Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities\n",
      "from operator import itemgetter\n",
      "from gensim.utils import simple_preprocess \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load Stopword List"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist=[\"a\",\"able\",\"about\",\"across\",\"after\",\"all\",\"try\",\"bye\",\"almost\",\"also\",\"am\",\"among\",\"an\",\"and\",\"any\",\"are\",\"as\",\"at\",\"be\",\"because\",\"been\",\"but\",\"by\",\"can\",\"cannot\",\"could\",\"dear\",\"did\",\"do\",\"does\",\"either\",\"else\",\"ever\",\"every\",\"for\",\"from\",\"get\",\"got\",\"had\",\"has\",\"have\",\"he\",\"her\",\"hers\",\"him\",\"his\",\"how\",\"however\",\"i\",\"if\",\"in\",\"into\",\"is\",\"it\",\"its\",\"just\",\"least\",\"let\",\"like\",\"likely\",\"may\",\"me\",\"might\",\"most\",\"must\",\"my\",\"neither\",\"new\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"often\",\"on\",\"only\",\"or\",\"other\",\"our\",\"own\",\"rather\",\"said\",\"say\",\"says\",\"she\",\"should\",\"since\",\"so\",\"some\",\"than\",\"that\",\"the\",\"their\",\"them\",\"then\",\"there\",\"these\",\"they\",\"this\",\"tis\",\"to\",\"too\",\"twas\",\"us\",\"wants\",\"was\",\"we\",\"were\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"would\",\"yet\",\"you\",\"your\"]\n",
      "from gensim.parsing.preprocessing import STOPWORDS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load NYTimes Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Replace the filename with the corresponding location in your computer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "nyt_data = []\n",
      "with open('/Users/pralav/Documents/Projects/Owned/DeepWiki/static/NYTimes.csv') as f:\n",
      "    lines=f.readlines()\n",
      "for line in lines:\n",
      "    data=filter(None,line.split(\";\"))\n",
      "    data[2]=data[2].replace('\"','')\n",
      "    data[2]=data[2].replace('.','')\n",
      "    data[2]=data[2].replace(':','')\n",
      "    \n",
      "    nyt_data.append(data[2].replace('\"',''))\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Split the document into words and Remove Stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize(text):\n",
      "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
      "documents = [document for document in nyt_data]\n",
      "# texts = [[word for word in document.lower().split() if word not in stoplist and not word.isdigit()]\n",
      "#          for document in documents]\n",
      "texts=[tokenize(doc) for doc in documents]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Create Vocabulary from the text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary = corpora.Dictionary(texts)\n",
      "dictionary.filter_extremes(no_below=5, no_above=0.2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Vectorize the document based on words in Vocabulary"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [dictionary.doc2bow(text) for text in texts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Create TFIDF Vectors of the corpus created"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = models.TfidfModel(corpus)\n",
      "corpus_tfidf = tfidf[corpus]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Create LDA Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_topics = 3\n",
      "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=n_topics,iterations=500,passes=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Show Topics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(0, n_topics):\n",
      "    temp = lda.show_topic(i, 10)\n",
      "    terms = []\n",
      "    for term in temp:\n",
      "        terms.append(term[1])\n",
      "    print \"Top 10 terms for topic #\" + str(i) + \": \"+ \", \".join(terms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 10 terms for topic #0: iraq, president, bush, report, city, campaign, nation, special, challenged, gop\n",
        "Top 10 terms for topic #1: war, new, iraq, clinton, baghdad, chief, nation, attacks, attack, baseball\n",
        "Top 10 terms for topic #2: case, says, politics, threats, court, race, responses, vote, home, day\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Another Example : 20 news groups data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from pprint import pprint\n",
      "import re\n",
      "from gensim.models import ldamodel, TfidfModel\n",
      "data = fetch_20newsgroups(subset='train')\n",
      "pprint(list(data.target_names))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['alt.atheism',\n",
        " 'comp.graphics',\n",
        " 'comp.os.ms-windows.misc',\n",
        " 'comp.sys.ibm.pc.hardware',\n",
        " 'comp.sys.mac.hardware',\n",
        " 'comp.windows.x',\n",
        " 'misc.forsale',\n",
        " 'rec.autos',\n",
        " 'rec.motorcycles',\n",
        " 'rec.sport.baseball',\n",
        " 'rec.sport.hockey',\n",
        " 'sci.crypt',\n",
        " 'sci.electronics',\n",
        " 'sci.med',\n",
        " 'sci.space',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        " 'talk.politics.mideast',\n",
        " 'talk.politics.misc',\n",
        " 'talk.religion.misc']\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Split Words, Create Dictionary, Remove Stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#documents = [[w.lower() for w in re.findall('[a-zA-Z]{4,}', doc)] for doc in data.data]\n",
      "documents= [tokenize(doc) for doc in data.data]\n",
      "documents = [d[:500] for d in documents]\n",
      "dictionary = corpora.Dictionary(documents)\n",
      "\n",
      "\n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1 and len(dictionary[tokenid])<=2]\n",
      "dictionary.filter_tokens(stop_ids + once_ids)\n",
      "dictionary.filter_extremes(no_below=20, no_above=0.1)\n",
      "dictionary.compactify()\n",
      "\n",
      "corpus = [dictionary.doc2bow(text) for text in documents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "TFIDF from data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = TfidfModel(corpus)\n",
      "corpus_tfidf = tfidf[corpus]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Create LDA Model from Corpus and Print Topics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda = ldamodel.LdaModel(corpus_tfidf, num_topics=10,id2word=dictionary,iterations=1000, passes=5)\n",
      "i = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i=0\n",
      "for topic in lda.show_topics(num_topics=10, formatted=False,num_words=10):\n",
      "    i = i + 1\n",
      "    print \"Topic #\" + str(i) + \":\"\n",
      "    c=0\n",
      "    for p, id in topic:\n",
      "        print id,\n",
      "    print \"\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic #1:\n",
        "windows ibm uk sun pitt ac graphics window hp gordon \n",
        "Topic #2:\n",
        "sale car bike price purdue ac looking ohio dod power \n",
        "Topic #3:\n",
        "key government netcom law gun god clipper encryption chip public \n",
        "Topic #4:\n",
        "space alaska ti henry stratus georgia nasa moon wpi dseg \n",
        "Topic #5:\n",
        "israel israeli arab uci jews arabs mcgill lebanese israelis orion \n",
        "Topic #6:\n",
        "mot harris eos intercon utexas ncsu amanda larc sdsu rtsg \n",
        "Topic #7:\n",
        "vesa lc fi fpu maine handheld msstate window ericsson tw \n",
        "Topic #8:\n",
        "team game hockey players games baseball year play season nhl \n",
        "Topic #9:\n",
        "drive card mb windows scsi disk apple mouse drivers video \n",
        "Topic #10:\n",
        "god jesus bible church caltech virginia keith sgi christ men \n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}